{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hl9vY71OoLFe"
   },
   "source": [
    "<h3> Run the TensorFlow sequence to sequence model tutorial on a custom dataset of favorite songs to gneerate new lyrics. The approach closely follows the tutorial on Tensorflow by Google (https://www.tensorflow.org/tutorials/sequences/text_generation) which in turn works with the Shakespeare dataset. Here I work with my own.\n",
    "\n",
    "<h5> Our model will train on a GPU in Google Collab or on a CPU if one is not available. To run on Google Collab, make sure the data is in the correct directory. \n",
    "\n",
    "<h5> First I tried to run the model on a simple song lyrics dataset which I created consisting of only lyrics from one album from the band The National. The model outputs garbage, so I augment the dataset using all songs lyrics from the indie genre from the Kaggle dataset https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics, and retrain the model\n",
    "\n",
    "<h5> The file \"Extract_Song_Lyrics.ipynb\" extracts the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hsABVsDvoLFg",
    "outputId": "27630e52-4fc1-4cd2-bc66-815c73d810ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "print(tf.VERSION)\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cQmVlQioLFn"
   },
   "source": [
    "<h4> Check if GPU is available, if not use CPU hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x76YhL2goLFp",
    "outputId": "73137c80-1d7a-49c6-bd2d-6de73c085242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Device /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if not tf.test.gpu_device_name():\n",
    "        print(\"Please train model on GPU\")\n",
    "else:\n",
    "    print('GPU Device {}'.format(tf.test.gpu_device_name()))\n",
    "    device_name = tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lJvc-kXOoLFv",
    "outputId": "dd9bfd6d-c19a-421a-aa14-a4de1bb7213d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE PATH = /content/drive/My Drive/deep_learning_models/Seq_2_Seq_Models\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(os.getcwd(), 'lyrics.txt')\n",
    "print(\"BASE PATH = {}\".format(BASE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4XXxpuLoLF0"
   },
   "source": [
    "<h3> Now we have a much larger text dataset composed of over 3 million characters. To train this, we should probably use a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCNVD6BxouVh"
   },
   "source": [
    "<h3> To run code in Google Collab, we first need to mount the content directory and browse it to locate the data. Since the dataset file is quite small, I have loaded into my Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4d9TtaL1pH14"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "\n",
    "#uploaded = files.upload()\n",
    "\n",
    "#for fn in uploaded.keys():\n",
    "#  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GLn9-ssarBgD",
    "outputId": "ffece8ab-7482-4978-c633-0dae190f36ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "2i3hBafnsi21",
    "outputId": "c0fece10-2b10-4634-8ef9-a2aa1108ac42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 380000-lyrics-from-metrolyrics       main.ipynb\n",
      " 380000-lyrics-from-metrolyrics.zip   main.py\n",
      " cleaned_lyrics.txt\t\t      song_lyrics.txt\n",
      " Extract_Song_Lyrics.ipynb\t      Text_Gen_Songs.ipynb\n",
      " Extract_Song_Lyrics.py\t\t      Text_Gen_Songs.py\n",
      " lyrics.txt\t\t\t     'training_checkpoints)'\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/content/drive/My Drive/deep_learning_models/Seq_2_Seq_Models')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8Ev1OzBeoLF2",
    "outputId": "94ff56eb-8f0e-4cdc-c72d-d307dd4cf83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 3176445 characters\n"
     ]
    }
   ],
   "source": [
    "text = open('cleaned_lyrics.txt', encoding = 'ISO-8859-1').read().replace('\\n', ' \\n ')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSiG2Zc0oLF7"
   },
   "source": [
    "<h5> Compared to the Shakespeare dataset which has over 1,000,000 characters, the augmented dataset I now use which consists of all song lyrics from indie genres contains over 3 million characters. Makes sense to train this on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VXIu_bVwoLF9",
    "outputId": "7ba97a70-b100-4cca-daea-f490f2c59283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RfK5s8foLGE"
   },
   "source": [
    "<h5> Create a lookup table which maps characters to numbers and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YD7G79JooLGH"
   },
   "outputs": [],
   "source": [
    "char2idx = {char:i for i, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUNXY2RKoLGK"
   },
   "source": [
    "<h5> Each unique character is now mapped to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "9_KZ8kH0oLGL",
    "outputId": "f9a4f92d-3a6a-4fe3-d860-5fd8f92e83f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' '    --->    0\n",
      "'!'    --->    1\n",
      "'\"'    --->    2\n",
      "\"'\"    --->    3\n",
      "','    --->    4\n",
      "'.'    --->    5\n",
      "'A'    --->    6\n",
      "'B'    --->    7\n",
      "'C'    --->    8\n",
      "'D'    --->    9\n",
      "'E'    --->   10\n",
      "'F'    --->   11\n",
      "'G'    --->   12\n",
      "'H'    --->   13\n",
      "'I'    --->   14\n",
      "'J'    --->   15\n",
      "'K'    --->   16\n",
      "'L'    --->   17\n",
      "'M'    --->   18\n",
      "'N'    --->   19\n"
     ]
    }
   ],
   "source": [
    "for char,_ in zip(char2idx, range(0, 20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DUn09JkvoLGQ",
    "outputId": "fa559943-38dc-4be7-e3d0-fd9519595e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Don't feel so bad,\\nIt's ---- characters mapped to int ---- > [ 2  9 47 46  3 52  0 38 37 37 44  0 51 47  0 34 33 36  4 32 46 14 52  3\n",
      " 51]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(text[:25], text_as_int[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fG8uf5spyEF9"
   },
   "source": [
    "<h4> Let's look at the dataset to see what kind of text the model contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FQPgMW5cx5bW",
    "outputId": "02a6daa3-456a-4d0e-9548-fe1667a43c57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Don\\'t feel so bad,\\\\nIt\\'s'"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJsdzkkxx7yo"
   },
   "source": [
    "*The  text contains things like <x4>which mean to repeat the lyric 4 times. How will our model deal with this kind of text input?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWwA0K3aoLGX"
   },
   "source": [
    "#### Next we start training the model. This is a character based model, so given a sequence of characters of length seq_lgth, the model predicts the next character. To train such a model, we pick a fixed sequence length for training, and train the model to predict the same sequence shifted by 1. So given the word \"cussin\" above, if the input is \"cussi\" the target is \"ussin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "fslaFYqPoLGZ",
    "outputId": "83f71214-ba15-49c3-9fa2-2b37c0300c36",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\"Don\\'t feel so bad,\\\\nIt\\'s just the way the wheel turns\\\\nStay in where it\\'s quiet,\\\\nAnd where the sick'\n",
      "\" burns\\\\nAnd I never meant to make you feel alone,\\\\nI never meant to hide\\\\nAnd I never thought I'd mak\"\n",
      "\"e you see the light,\\\\nBefore it was your time\\\\nI'm losing you\\\\nI'm losing you\\\\nBack straight, arms do\"\n",
      "\"wn,\\\\nLift the weight off your shoulders\\\\nTake your time, take it slow\\\\nGet ready for the end\\\\nI'm los\"\n",
      "\"ing you\\\\nI'm losing you\\\\nLight head\\\\nCold sweat\\\\nFind the vein\\\\nAnd deliver x\\\\nI'm losing you\\\\nI'm lo\"\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100 # This should be a parameter we can play with. Why is 100 good for songs?\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "# create training and target examples:\n",
    "\n",
    "chunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder= True)\n",
    "#chunks = chunks.apply(tf.contrib.data.batch_and_drop_remainder(seq_length+1))\n",
    "for item in chunks.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()]))) # convert idc2char to a word, use repr to make printable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IiH2n9KZoLGd"
   },
   "source": [
    "<h5> This function creates inputs and target sequences for the model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gC1fi4bAoLGf"
   },
   "outputs": [],
   "source": [
    "def input_target(chunk):\n",
    "    input_text = chunk[:-1] # all but last\n",
    "    target_text = chunk[1:] # all but first\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGpMdovfoLGi"
   },
   "outputs": [],
   "source": [
    "def create_dataset(seq_length, text_data):\n",
    "    ''' Given the text data and a sequence length, this creates a new dataset which consists of input and target chunks'''\n",
    "    chunks = tf.data.Dataset.from_tensor_slices(text_data).batch(seq_length+1, drop_remainder= True)\n",
    "    return chunks.map(input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TilHTQwoLGl"
   },
   "outputs": [],
   "source": [
    "dataset = create_dataset(seq_length, text_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZowPL8eoLGs"
   },
   "source": [
    "<h5> Look at some examples of input and target sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Ae1JoVULoLGu",
    "outputId": "4ab71be9-f801-48d4-d76f-6476dc87f289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\"Don\\'t feel so bad,\\\\nIt\\'s just the way the wheel turns\\\\nStay in where it\\'s quiet,\\\\nAnd where the sic'\n",
      "\"Don't feel so bad,\\\\nIt's just the way the wheel turns\\\\nStay in where it's quiet,\\\\nAnd where the sick\"\n",
      "\" burns\\\\nAnd I never meant to make you feel alone,\\\\nI never meant to hide\\\\nAnd I never thought I'd ma\"\n",
      "\"burns\\\\nAnd I never meant to make you feel alone,\\\\nI never meant to hide\\\\nAnd I never thought I'd mak\"\n",
      "\"e you see the light,\\\\nBefore it was your time\\\\nI'm losing you\\\\nI'm losing you\\\\nBack straight, arms d\"\n",
      "\" you see the light,\\\\nBefore it was your time\\\\nI'm losing you\\\\nI'm losing you\\\\nBack straight, arms do\"\n",
      "\"wn,\\\\nLift the weight off your shoulders\\\\nTake your time, take it slow\\\\nGet ready for the end\\\\nI'm lo\"\n",
      "\"n,\\\\nLift the weight off your shoulders\\\\nTake your time, take it slow\\\\nGet ready for the end\\\\nI'm los\"\n",
      "\"ing you\\\\nI'm losing you\\\\nLight head\\\\nCold sweat\\\\nFind the vein\\\\nAnd deliver x\\\\nI'm losing you\\\\nI'm l\"\n",
      "\"ng you\\\\nI'm losing you\\\\nLight head\\\\nCold sweat\\\\nFind the vein\\\\nAnd deliver x\\\\nI'm losing you\\\\nI'm lo\"\n"
     ]
    }
   ],
   "source": [
    "for input_dataset, target_dataset in dataset.take(5):\n",
    "    print(repr(''.join(idx2char[input_dataset.numpy()])))\n",
    "    print(repr(''.join(idx2char[target_dataset.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YySy6B9RoLG7"
   },
   "source": [
    "<h4> Training\n",
    "\n",
    "<h5> We used tf.data to break up the data into sequences, now we must buffer the data and load it in batches for training. This is easily done with batch command and shuffle command available with a tf.Dataset object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sfNBS6OUoLG9",
    "outputId": "59f8cc60-5338-4ee0-c186-5f775b7d2604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n"
     ]
    }
   ],
   "source": [
    "# batch size and shuffle dataset\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rJucwsyJoLHG",
    "outputId": "4d54a196-7bf1-4582-922d-2a774cfce1b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7FtV0zeoLHN"
   },
   "source": [
    "### Model:\n",
    "\n",
    "<h4> We use keras.Sequential to define the model. If a GPU is availabble, we train using CuDNNGRU or else use a GRU.\n",
    "<h4> The model consists of three layers: Input of dimension (batch_size,)\n",
    "<h5>    a) an embedding layer [batch_size, embedding_size]\n",
    "<h5>    b) a GRU for training [batch_size, rnn_units]\n",
    "<h5>    c) a fully connected dense layer which maps the output of the GRU to a vector for training [batch_size, vocab_size]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkS53YrjoLHO"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "rnn_units = 1024\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFgKNkOFoLHS"
   },
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation = 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ci4RgwsvoLHV"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_size, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None]),\n",
    "        \n",
    "        rnn(rnn_units,\n",
    "           return_sequences = True,\n",
    "           recurrent_initializer = 'glorot_uniform',\n",
    "           stateful = True), #retain the state of the RNN as the model learns the context from batch to batch\n",
    "       tf.keras.layers.Dense(vocab_size) \n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJQ8X6wRoLHY"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "jbezvpQjoLHb",
    "outputId": "e4c43cdc-8b9d-4b8f-dff4-80e822df2c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (64, None, 256)           15104     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_11 (CuDNNGRU)      (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (64, None, 59)            60475     \n",
      "=================================================================\n",
      "Total params: 4,013,883\n",
      "Trainable params: 4,013,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DThddkomoLHg"
   },
   "source": [
    "<h5> With Keras its super easy to build a model using Keras.Sequential Let's test it out. Check if the model outputs look reasaonble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-eMv5y0410U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M2KU6vbdoLHg",
    "outputId": "6ca59c0b-cbad-4424-fe9d-d8f06f5abf95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 59) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "VFqK_VNGoLHp",
    "outputId": "342ca792-e783-4f4a-ba82-ee0ab3b51ef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'ke it there\\\\nSo you can move your court case\\\\nWay across town\\\\nYou can move it across the whole wide'\n",
      "\n",
      "Output: \n",
      " 'oJJfXunqUcNcSR.zZV.YootNZSdxjFwBatpPvNIxuF!uJGHTZBM\"yoXykKj\"ohpfzgoRvEWGHb\\\\.gssS.sqAi\\'npfVyck\\\\xcqpyt'\n"
     ]
    }
   ],
   "source": [
    "# draws a sample from multinomial distribution of length example_batch_predictions[0], and 1 sample\n",
    "sampled_indices = tf.multinomial(example_batch_predictions[0], num_samples = 1) \n",
    "sampled_indices = tf.squeeze(sampled_indices, axis = -1).numpy() \n",
    "\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Output: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYJvJmgmoLHy"
   },
   "source": [
    "<h5> As you can see the untrained model outputs a bunch of garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "24Y3dHRDoLH0"
   },
   "source": [
    "<h3> Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3umb1osoLH2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "             loss = tf.losses.sparse_softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_1JGa9moLH8"
   },
   "outputs": [],
   "source": [
    "model.build(tf.TensorShape([BATCH_SIZE, seq_length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_vebvw7oLIE"
   },
   "source": [
    "<h5> Create a directory to save model checkpoints and make sure the directory is created each time model parameters are changed and updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afC8QaTroLIG"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'training_checkpoints)')\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_kRHVgo6oLIO",
    "outputId": "731002d4-4b6a-4ac6-a68c-c724f0e53829"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 193,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLB0AuZpCM7X"
   },
   "source": [
    "<h4> Looking at the evolution of the loss, should stop training at about 25 epochs beyond which the model begins to overfit and the loss goes back up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "va6nTf_XoLIb",
    "outputId": "057c5739-5540-441f-a1a1-704afaca9386",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "496/496 [==============================] - 78s 157ms/step - loss: 2.0588\n",
      "Epoch 2/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.4975\n",
      "Epoch 3/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.3679\n",
      "Epoch 4/20\n",
      "496/496 [==============================] - 78s 157ms/step - loss: 1.2940\n",
      "Epoch 5/20\n",
      "496/496 [==============================] - 78s 157ms/step - loss: 1.2371\n",
      "Epoch 6/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.1893\n",
      "Epoch 7/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.1461\n",
      "Epoch 8/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.1063\n",
      "Epoch 9/20\n",
      "496/496 [==============================] - 78s 157ms/step - loss: 1.0749\n",
      "Epoch 10/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0490\n",
      "Epoch 11/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0303\n",
      "Epoch 12/20\n",
      "496/496 [==============================] - 77s 155ms/step - loss: 1.0166\n",
      "Epoch 13/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0086\n",
      "Epoch 14/20\n",
      "496/496 [==============================] - 78s 156ms/step - loss: 1.0029\n",
      "Epoch 15/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0019\n",
      "Epoch 16/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0008\n",
      "Epoch 17/20\n",
      "496/496 [==============================] - 78s 156ms/step - loss: 1.0015\n",
      "Epoch 18/20\n",
      "496/496 [==============================] - 78s 156ms/step - loss: 1.0068\n",
      "Epoch 19/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0136\n",
      "Epoch 20/20\n",
      "496/496 [==============================] - 77s 156ms/step - loss: 1.0212\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs = EPOCHS, steps_per_epoch=steps_per_epoch,callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNce64y5oLIl"
   },
   "source": [
    "<h2> Generate New Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFDH0W31oLIn"
   },
   "source": [
    "<h3> Test the Model:\n",
    "\n",
    "<h4> Restore the latest checkpoint. To keep predictions simple, make sure the model is rebuilt with a batch size of 1 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RoDnW4o3oLIq",
    "outputId": "951cbbc7-a852-433b-c49a-b38116dff90d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/drive/My Drive/deep_learning_models/Seq_2_Seq_Models/training_checkpoints)/ckpt_20'"
      ]
     },
     "execution_count": 195,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUQ62XOAoLI0"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size = 1)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_prefix))\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "t_RRyzA0oLI-",
    "outputId": "d05ccc7d-e13c-4f1f-d38c-5cbda19200a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (1, None, 256)            15104     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_12 (CuDNNGRU)      (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (1, None, 59)             60475     \n",
      "=================================================================\n",
      "Total params: 4,013,883\n",
      "Trainable params: 4,013,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1wEGnqzoLJG"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 200\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 0.1\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=5)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9VK4ElfQoLJM",
    "outputId": "17810c4f-6b51-47e9-c6d9-4c061db3f259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simona song when I'm down \n",
      " And if I fall asleep to be found \n",
      " I can feel it deep in my head \n",
      " And the wind blows her way \n",
      " And you say the world can make me want to be \n",
      " It was the one who loves you the most, y\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string='Simona').replace('\\\\n', ' \\n '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMwsY8rroLJV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WjUBZJ2pC6X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
